{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def extract_business_links_and_names():\n",
    "    business_links = driver.find_elements(By.XPATH, \"//a[contains(@href, '/Toolkit/Jobs/find-businesses-detail.aspx?')]\")\n",
    "    for link in business_links:\n",
    "        business_finder_url = link.get_attribute('href')\n",
    "        business_name = link.text\n",
    "        business_finder_urls.append(business_finder_url)\n",
    "        businessNames.append(business_name)\n",
    "industry_names = [\n",
    "    \"110000\",\n",
    "    \"210000\",\n",
    "    \"220000\",\n",
    "    \"230000\",\n",
    "    \"310000\",\n",
    "    \"320000\",\n",
    "    \"330000\",\n",
    "    \"420000\",\n",
    "    \"440000\",\n",
    "    \"450000\",\n",
    "    \"480000\",\n",
    "    \"490000\",\n",
    "    \"510000\",\n",
    "    \"520000\",\n",
    "    \"530000\",\n",
    "    \"540000\",\n",
    "    \"550000\",\n",
    "    \"560000\",\n",
    "    \"610000\",\n",
    "    \"620000\",\n",
    "    \"710000\",\n",
    "    \"720000\",\n",
    "    \"810000\",\n",
    "    \"920000\"\n",
    "]\n",
    "page_limits = [\n",
    "    1154, 1080, 199, 10672, 1551\n",
    "]\n",
    "options = webdriver.ChromeOptions()\n",
    "#options.add_argument(\"--headless\")\n",
    "#options.add_argument(\"--disable-gpu\")\n",
    "#options.add_argument(\"--no-sandbox\")\n",
    "#options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "curPage = 1\n",
    "for industry_name in industry_names:\n",
    "    business_finder_urls = []\n",
    "    businessNames = []\n",
    "    while True:\n",
    "        url = \"https://www.careeronestop.org/Toolkit/Jobs/find-businesses-results.aspx?curPage=\"+str(curPage)+\"&keyword=\"+industry_name+\"&location=California&lang=en&pagesize=50\"\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/Toolkit/Jobs/find-businesses-detail.aspx?')]\")))\n",
    "            extract_business_links_and_names()\n",
    "            curPage += 1\n",
    "        except:\n",
    "            curPage = 1\n",
    "            break\n",
    "    data = {\n",
    "    \"Business Name\": businessNames,\n",
    "    \"Business Finder Website\": [site for site in business_finder_urls],\n",
    "    \"Industry\": industry_name\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(industry_name + \".xlsx\", index=False)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = [\n",
    "    \"11 - Agriculture, Forestry, Fishing and Hunting\",\n",
    "    \"21 - Mining, Quarrying, and Oil and Gas Extraction\",\n",
    "    \"22 - Utilities\",\n",
    "    \"23 - Construction\",\n",
    "    \"31-33 - Manufacturing\",\n",
    "    \"42 - Wholesale Trade\",\n",
    "    \"44-45 - Retail Trade\",\n",
    "    \"48-49 - Transportation and Warehousing\",\n",
    "    \"51 - Information\",\n",
    "    \"52 - Finance and Insurance\",\n",
    "    \"53 - Real Estate and Rental and Leasing\",\n",
    "    \"54 - Professional, Scientific, and Technical Services\",\n",
    "    \"55 - Management of Companies and Enterprises\",\n",
    "    \"56 - Administrative and Support and Waste Management and Remediation Services\",\n",
    "    \"61 - Educational Services\",\n",
    "    \"62 - Health Care and Social Assistance\",\n",
    "    \"71 - Arts, Entertainment, and Recreation\",\n",
    "    \"72 - Accommodation and Food Services\",\n",
    "    \"81 - Other Services (except Public Administration)\",\n",
    "    \"92 - Public Administration\"\n",
    "]\n",
    "initial = pd.DataFrame()\n",
    "for file in list_files:\n",
    "    df = pd.read_excel(\"Raw Data/\" + file + \".xlsx\")\n",
    "    initial = pd.concat([initial, df], ignore_index = True)\n",
    "initial = initial.drop_duplicates()\n",
    "initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "def extract_details(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'cos-table-detail'))\n",
    "        )\n",
    "        table = driver.find_element(By.CLASS_NAME, 'cos-table-detail')\n",
    "        industry_code = None\n",
    "        website = None\n",
    "        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(cells) > 1:\n",
    "                if 'Industry Code' in cells[0].text:\n",
    "                    industry_code = cells[1].text.strip()[:2]  # Get first 2 digits\n",
    "                elif 'Website' in cells[0].text:\n",
    "                    website = cells[1].find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        return industry_code, website\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {url}: {e}\")\n",
    "    return None, None\n",
    "final = initial.copy()\n",
    "industry_names = [\"11\", \"21\", \"22\", \"23\", \"31\", \"32\", \"33\", \"42\", \"44\", \"45\", \"48\", \"49\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"61\", \"62\", \"71\", \"72\", \"81\", \"92\"]\n",
    "industry_dict = {code: 0 for code in industry_names}\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "industry_codes = []\n",
    "business_urls = []\n",
    "names = []\n",
    "final[\"Sampled\"] = False\n",
    "count = 0\n",
    "while count < 20 * 150:\n",
    "    random_index = np.random.choice(final.index, size=1, replace=False)[0]\n",
    "    if final[\"Sampled\"][random_index] == False:\n",
    "        final.at[random_index, \"Sampled\"]= True\n",
    "        business_finder_url = final['Business Finder Website'][random_index]\n",
    "        industry_code, website = extract_details(driver, business_finder_url)\n",
    "        if website is not None:\n",
    "            count += 1\n",
    "            names.append(final['Business Name'][random_index])\n",
    "            industry_codes.append(industry_code)\n",
    "            business_urls.append(website)\n",
    "            #industry_dict[industry_code] += 1\n",
    "driver.quit()\n",
    "sample = pd.DataFrame({'Name': names, 'Industry Code': industry_codes, 'Business Homepage': business_urls})\n",
    "sample.to_excel(\"accurate_random_sample_2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from boilerpy3 import extractors\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_final_url(url, headers):\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, allow_redirects=True)\n",
    "        return response.url\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return url\n",
    "    \n",
    "def f2(g):\n",
    "        priorities = [\"privacy policy\", \"privacy\", \"policy\", \"privacy center\", \"privacy statement\", \"legal\",\n",
    "                      \"legal notice\", \"privacy notice\", \"data protection\", \"data privacy\",\n",
    "                      \"privacy and terms\", \"security and privacy\", \"privacy & security\"\n",
    "                      \"confidentiality\", \"gdpr compliance\", \"privacy practices\"]\n",
    "        for key in priorities:\n",
    "            if key in g:\n",
    "                return g.get(key)\n",
    "        return None\n",
    "\n",
    "def get_privacy_policy_link(homepage_url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    homepage_url = get_final_url(homepage_url, headers)\n",
    "    try:\n",
    "        response = requests.get(homepage_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 406:\n",
    "            print(f\"406 Not Acceptable error for {homepage_url}\")\n",
    "            return \"Error fetching homepage\", homepage_url\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"403 Forbidden error for {homepage_url}\")\n",
    "            return \"Error fetching homepage\", homepage_url\n",
    "        else:\n",
    "            print(f\"HTTP error for {homepage_url}: {e}\")\n",
    "        return \"Error fetching homepage\", homepage_url\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {homepage_url}: {e}\")\n",
    "        return \"Error fetching homepage\", homepage_url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    dict = {}\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'privacy' in link.text.lower() or 'policy' in link.text.lower() or 'legal' in link.text.lower():\n",
    "            curr = link['href']\n",
    "            full_url = requests.compat.urljoin(homepage_url, curr)\n",
    "            if curr != 'https://policies.google.com/privacy' and full_url != homepage_url:\n",
    "                dict.update({link.text.lower():full_url})\n",
    "    return f2(dict), homepage_url\n",
    "\n",
    "def extract_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 406:\n",
    "            print(f\"406 Not Acceptable error for {url}\")\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"403 Forbidden error for {url}\")\n",
    "            return handle_forbidden_error(url)\n",
    "        else:\n",
    "            print(f\"HTTP error for {url}: {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def handle_forbidden_error(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Still getting HTTP error for {url}: {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url} with modified user-agent: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def check_mentions(url, keywords):\n",
    "    try:\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html = urlopen(req).read()\n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "        text = soup.get_text()\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        ans = []\n",
    "        text = text.lower()\n",
    "        keywords = [k.lower() for k in keywords]\n",
    "        for k in keywords:\n",
    "            if k in text:\n",
    "                indeces = [m.start() for m in re.finditer(k, text)]\n",
    "                for i in indeces:\n",
    "                    if text[i - 1] != '\\n' and text[i+len(k)] != '\\n':\n",
    "                        ans.append(k.capitalize())\n",
    "                        break\n",
    "        return ans\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df = pd.read_excel(\"accurate_random_sample_2.xlsx\") # accurate_random_sample_2\n",
    "homepages = df['Business Homepage']\n",
    "privacy_policy_links = []\n",
    "gdpr_mentioned = []\n",
    "colorado_mentioned = []\n",
    "other_states_mentioned = []\n",
    "\n",
    "for i, homepage_url in enumerate(homepages):\n",
    "    privacy_policy_link, final_homepage_url = get_privacy_policy_link(homepage_url)\n",
    "    df.at[i, 'Business Homepage'] = final_homepage_url\n",
    "    if privacy_policy_link == \"Error fetching homepage\":\n",
    "        privacy_policy_links.append(\"Error fetching homepage\")\n",
    "        gdpr_mentioned.append(\"Error fetching homepage\")\n",
    "        colorado_mentioned.append(\"Error fetching homepage\")\n",
    "        other_states_mentioned.append(\"Error fetching homepage\")\n",
    "    elif privacy_policy_link:\n",
    "        mentions = check_mentions(privacy_policy_link, ['Gdpr', 'General data protection regulation', 'Colorado', \"Alabama\", \"Alaska\", \"American Samoa\", \"Arizona\", \"Arkansas\", \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Minor Outlying Islands\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Northern Mariana Islands\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"U.S. Virgin Islands\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"])\n",
    "        if mentions != \"Boilerplate error\":\n",
    "            privacy_policy_links.append(privacy_policy_link)\n",
    "            gdpr_mentioned.append('Gdpr' in mentions or 'General data protection regulation' in mentions)\n",
    "            colorado_mentioned.append('Colorado' in mentions)\n",
    "            to_remove = ['Gdpr', 'General data protection regulation', 'Colorado']\n",
    "            for m in list(mentions):\n",
    "                if m in to_remove:\n",
    "                    mentions.remove(m)\n",
    "            other_states_mentioned.append(', '.join(mentions))\n",
    "        else:\n",
    "            privacy_policy_links.append(privacy_policy_link)\n",
    "            gdpr_mentioned.append('Error fetching privacy policy page')\n",
    "            colorado_mentioned.append('Error fetching privacy policy page')\n",
    "            other_states_mentioned.append('Error fetching privacy policy page')\n",
    "    else:\n",
    "        privacy_policy_links.append(\"Homepage does not link to privacy page\")\n",
    "        gdpr_mentioned.append('')\n",
    "        colorado_mentioned.append('')\n",
    "        other_states_mentioned.append('')\n",
    "\n",
    "df['Privacy Policy Link'] = privacy_policy_links\n",
    "df['GDPR Mentioned'] = gdpr_mentioned\n",
    "df['Colorado Mentioned'] = colorado_mentioned\n",
    "df['Other States Mentioned'] = other_states_mentioned\n",
    "df.to_excel(\"accurate_random_sample_2_1.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"accurate_random_sample_2_1.xlsx\")\n",
    "df = df.dropna(subset=[\"Privacy Policy Link\"])\n",
    "df = df.drop_duplicates(subset=[\"Business Homepage\"], keep=False)\n",
    "df = df[df['Privacy Policy Link'] != 'Error fetching homepage']\n",
    "df = df[df['GDPR Mentioned'] != 'Error fetching privacy policy page']\n",
    "#df[\"GDPR Mentioned\"] = df[\"GDPR Mentioned\"].fillna(False)\n",
    "#df[\"Colorado Mentioned\"] = df[\"Colorado Mentioned\"].fillna(False)\n",
    "df.to_excel(\"Cleaned Data v3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(g):\n",
    "        priorities = [\"privacy policy\", \"privacy\", \"policy\", \"privacy center\", \"privacy statement\", \"legal\",\n",
    "                      \"legal notice\", \"privacy notice\", \"data protection\", \"data privacy\",\n",
    "                      \"privacy and terms\", \"security and privacy\",\n",
    "                      \"confidentiality\", \"gdpr compliance\", \"privacy practices\"]\n",
    "        for key in priorities:\n",
    "            if key in g:\n",
    "                return g.get(key)\n",
    "        return None\n",
    "def get_privacy_policy_link_mobile(homepage_url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15A372 Safari/604.1\"}\n",
    "    homepage_url = get_final_url(homepage_url, headers)\n",
    "    try:\n",
    "        response = requests.get(homepage_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 406:\n",
    "            print(f\"406 Not Acceptable error for {homepage_url}\")\n",
    "            return \"Error fetching homepage\", homepage_url\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"403 Forbidden error for {homepage_url}\")\n",
    "            return \"Error fetching homepage\", homepage_url\n",
    "        else:\n",
    "            print(f\"HTTP error for {homepage_url}: {e}\")\n",
    "        return \"Error fetching homepage\", homepage_url\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {homepage_url}: {e}\")\n",
    "        return \"Error fetching homepage\", homepage_url\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    dict = {}\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'privacy' in link.text.lower() or 'policy' in link.text.lower() or 'legal' in link.text.lower():\n",
    "            curr = link['href']\n",
    "            full_url = requests.compat.urljoin(homepage_url, curr)\n",
    "            if curr != 'https://policies.google.com/privacy' and full_url != homepage_url:\n",
    "                dict.update({link.text.lower():full_url})\n",
    "    return f2(dict)\n",
    "df = pd.read_excel(\"Cleaned Data v4.xlsx\")\n",
    "mobile_policy_links = []\n",
    "mobile_policy_links_match = []\n",
    "temp = df[\"Business Homepage\"]\n",
    "privacy_policy_links = df[\"Privacy Policy Link\"]\n",
    "i = 0\n",
    "for t in temp:\n",
    "    t2 = get_privacy_policy_link_mobile(t)\n",
    "    mobile_policy_links.append(t2)\n",
    "    if t2 == privacy_policy_links[i]:\n",
    "        mobile_policy_links_match.append(True)\n",
    "    elif privacy_policy_links[i] == 'Homepage does not link to privacy page':\n",
    "        mobile_policy_links_match.append('')\n",
    "    else:\n",
    "        mobile_policy_links_match.append(False)\n",
    "    i += 1\n",
    "df[\"Privacy Page linked on Mobile Homepage\"] = mobile_policy_links\n",
    "df[\"Mobile Privacy Link matches Website Link\"] = mobile_policy_links_match\n",
    "df.to_excel(\"Cleaned Data v5.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Mobile Privacy Link matches Website Link\"] != '']\n",
    "df[\"Mobile Privacy Link matches Website Link\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "def check_mentions(url, keywords):\n",
    "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(req).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    ans = []\n",
    "    #text = text.lower()\n",
    "    #keywords = [k.lower() for k in keywords]\n",
    "    print(text)\n",
    "    for k in keywords:\n",
    "        if k in text:\n",
    "            indeces = [m.start() for m in re.finditer(k, text)]\n",
    "            for i in indeces:\n",
    "                prev = text[i - 1]\n",
    "                next = text[i + len(k)]\n",
    "                if prev != '\\n' and next != '\\n' and not prev.isupper() and not next.isupper():\n",
    "                    while prev != '\\n':\n",
    "                        \n",
    "                        prev -= 1\n",
    "                    ans.append(k)\n",
    "                    break\n",
    "    return ans\n",
    "check_mentions('https://www.artofsaving.com/privacy-policy', [\"GDPR\", \"Colorado\", \"Alabama\", \"Alaska\", \"American Samoa\", \"Arizona\", \"Arkansas\", \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Minor Outlying Islands\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Northern Mariana Islands\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"U.S. Virgin Islands\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"\"\"\n",
    "states = ['Gdpr', 'General data protection regulation', 'Colorado', \"Alabama\", \"Alaska\", \"American Samoa\", \"Arizona\", \"Arkansas\", \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Minor Outlying Islands\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Northern Mariana Islands\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"U.S. Virgin Islands\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
    "for state in states:\n",
    "    if state in text:\n",
    "        print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from boilerpy3 import extractors\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import string\n",
    "def get_visible_text(driver):\n",
    "    # Get all text nodes visible on the page\n",
    "    elements = driver.find_elements(By.XPATH, \"//*[not(self::script) and not(self::style) and text()]\")\n",
    "    visible_texts = []\n",
    "    for element in elements:\n",
    "        if element.is_displayed():\n",
    "            visible_texts.append(element.text)\n",
    "    return ' '.join(visible_texts)\n",
    "\n",
    "def check_mentions(driver, url, keywords):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'html'))\n",
    "        )\n",
    "        \n",
    "        # Get the visible text from the page\n",
    "        visible_text = get_visible_text(driver)\n",
    "        \n",
    "        # Check if any of the keywords are in the visible text\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in visible_text.lower():\n",
    "                return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {url}: {e}\")\n",
    "        return ''\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#print(check_mentions(driver, \"https://technowood.com/\", [\"cookie\", \"cookies\"]))\n",
    "df = pd.read_excel(\"With Mobile Links.xlsx\")\n",
    "t1 = df[\"Business Homepage\"]\n",
    "emp = []\n",
    "for i in t1:\n",
    "    emp.append(check_mentions(driver, i, [\"cookie\", \"cookies\", \"your privacy policy\", \"share my personal information\", \"sell my personal information\", \"sale of personal information\"]))\n",
    "df[\"Shares/Sells Information\"] = emp\n",
    "df.to_excel(\"With Pop-ups 2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
